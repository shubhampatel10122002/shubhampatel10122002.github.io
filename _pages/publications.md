---
<!-- layout: archive -->
title: "Work Experience"
permalink: /work-experience/
author_profile: true
---
<br>
<img width="175" height="75" src="https://shubhampatel10122002.github.io/images/s.png"/>
 <br>

<br>
<b>Lead Research Assistant (Technology), Stanford University, Palo Alto, CA </b> <br>
Professor Gerald Fuller and Doctor Vinny Chandran Suja(Postdoc at Harvard University) <br>
Sept 2023 -- Decmeber 2024, <a href="https://github.com/shubhampatel10122002/jsn_mlm/tree/main" target="_blank">[Link to Code]</a><br>

Trained a RoBERTa model on 70M+ SMILES strings using masked language modeling. Achieved 98% accuracy in capturing molecular embeddings. Optimized transformer architecture to encode SMILES effectively. Adapted the model for surface tension prediction via transfer learning and gradient-based tuning, achieving an R² of 0.912. The pipeline enabled rapid inference and robust performance, demonstrating transformers' utility in cheminformatics and computational material science.


<br>
<img width="175" height="75" src="https://shubhampatel10122002.github.io/images/mitlogo.png"/> <br>
<br>
<b>Research Assistant, Massachusetts Institute Of Technology, Cambridge, MA, (Hybrid) </b> <br>
Professor Ju Ni <br>
Oct 2024 -- Present, <a href="https://github.com/shubhampatel10122002/MIT-Image-Generation" target="_blank">[Link to Code]</a> <br>

We are making generative model that given few microstructre images at discrete scale will predict entire microstructure at all magnification from cm to nm (think of continous zoom). The key is making model understand the physical material properties of material. To start we have tried using cycleGAN and diffusion model, to enforce understanding of the physcial material properties. Now we are training the model to generate images at all scales. 


<br>
<img width="175" height="75" src="https://shubhampatel10122002.github.io/images/google-deepmind-logo.png"/> <br>
<br>
<b>Research Intern, Google DeepMind, (Remote) </b> <br>
Doctor Alicia Y. Tsai  <br>
Sep 2024 -- Present, <a href="https://github.com/shubhampatel10122002/Harikrishna_IDL/tree/main" target="_blank">[Link to Code]</a> <br>

Working on implicit deep learning models for financial time series predictions. Our research demonstrates that implicit model architectures significantly reduce computational operations during inference compared to state-of-the-art classical deep learning models. Furthermore, the implicit models required four times fewer parameters our example. We benchmarked our model against Microsoft's SeqSNN ( CPG-PE) and found that our implicit model outperforms it.
<br>

<img width="175" height="75" src="https://shubhampatel10122002.github.io/images/logo-variations-thumbnail-gold-blue.png"/> <br>
<br>
<b>Research Intern, UC Berkely DeepMind, (Remote) </b> <br>
Professor Laurent EI Ghaunoi   <br>
Sep 2024 -- Present, <a href="https://github.com/shubhampatel10122002/Harikrishna_IDL/tree/main" target="_blank">[Link to Code]</a> <br>

Working on deep equilibrium models (DEQs), specifically focusing on their advantages over LSTM (Long Short-Term Memory) networks. In our experiments, we have demonstrated that DEQs can deliver superior performance in terms of accuracy and computational efficiency. By leveraging implicit models and eliminating the need for recurrent connections, DEQs significantly reduce training time and memory usage compared to LSTMs, while maintaining or even improving prediction accuracy. 

Our results indicate that DEQs are highly effective for financial time series forecasting, where both precision and computational efficiency are critical. The model's ability to process longer sequences without compromising on speed or resource consumption sets it apart as a promising alternative to traditional deep learning approaches.
<br>

<img width="175" height="75" src="https://shubhampatel10122002.github.io/images/upenn.png"/> <br>
<br>
<b>Lead Research Assistant (Technology), University Of Pennslyvania , Philadelphia, PA, (Hybrid) </b> <br>
Professor Davatzikos Christos <br>
Oct 2024 -- Present, <a href="https://github.com/shubhampatel10122002/UPenn_LLM" target="_blank">[Link to Code]</a>t <br>

Developed a foundational large language model (LLM) to predict multi-organ imaging-derived phenotypes (IDPs), such as those derived from brain MRI, heart MRI, and eye OCT scans, based on genetic variations represented by sets of SNPs (Single Nucleotide Polymorphisms). So, essentially model analyzes genetic data to estimate how certain genetic variations may influence observable characteristics in medical imaging across different organs.

This model incorporates data from the UK Biobank, which provides a comprehensive dataset linking individual-level SNP data with organ-specific imaging traits for the same subjects. Additionally, summary-level Genome-Wide Association Studies (GWAS) data is integrated into the model, allowing it to leverage known associations between SNPs and specific traits. GWAS data is extracted from resources like the GWAS Catalog, enhancing the model’s ability to identify and utilize SNP-IDP relationships.
By combining individual-level SNP data with broader GWAS information, the model aims to predict imaging-derived phenotypes with improved accuracy, offering insights into the potential genetic underpinnings of multi-organ characteristics. This foundational approach could support research in understanding how genetics shape organ structure and function as seen through imaging, potentially informing personalized medicine and further genetic studies
This project bridges genomics and medical imaging by leveraging genetic data to predict imaging-derived phenotypes (IDPs), offering a groundbreaking way to understand how DNA variations influence organ traits. It has the potential to revolutionize personalized medicine by uncovering genetic markers linked to diseases, enabling early diagnosis and targeted treatments.
<br>

<img width="175" height="75" src="https://nishtha777.github.io/images/Flipkart-Logo.jpg"/> <br>
<br>
<b>Software Developmer Intern , Recommendations Team. Bengaluru </b> <br>
Flipkart <br>
May 2023 -- August 2023 <br>

* Acquired and applied tech stack including Aerospike, Elastic Search, and Spring Boot to effectively contribute to the development of functionality for recommendation systems <br>
* Performed migration and rebase of Java 8 code to Java 17 code <br>
* Personalised the shopping experience of customers by implementing Machine Learning algorithms like Logistic Regression, XGBoost, and Random Forest <br>
* Executed over 10 deployments utilizing Kubernetes, building Docker images in CI/CD pipelines, and implementing Helm Chart <br>
* Technologies used: Software Development, SQL, C/C++, Git, Spring Boot, RESTful API, MongoDB, Kubernetes, Docker, Mockito, Grafana, Confluence, Jira, Lens, Object-Oriented Programming (OOP), Operating Systems, Distributed Systems, Aerospike, Elasticsearch, Machine Learning, Java, Linux<br>

